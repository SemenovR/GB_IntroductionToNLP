{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 7. Модель Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментировать с моделью генерации текста, попробовать модель на словах, посмотреть что отработает лучше<br>\n",
    "Поэкспериментировать с переводом написать по вашим наблюдениям где он ошибается, попробовать изменить архитектуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Поэкспериментировать с моделью генерации текста, попробовать модель на словах, посмотреть что отработает лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация на буквах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'Blok_A._Spisokshkolnoy._Polnoe_Sobranie_Stihotvoreniyi.txt',\n",
    "    'Cvetaeva_M._Polnoe_Sobranie_Stihotvoreniyi.txt',\n",
    "    'Esenin_S._Spisokshkolnoy._Polnoe_Sobranie_Stihotvoreniyi.txt',\n",
    "    'Lermontov_M._Spisokshkolnoy._Mcyiri.txt',\n",
    "    'Nekrasov_N._Stihotvoreniya.txt',\n",
    "    'Pushkin_A._Mednyiyi_Vsadnik.txt',\n",
    "    'Pushkin_A._Ruslan_I_Lyudmila.txt',\n",
    "    'Pushkin_A._Spisokshkolnoy._Evgeniyi_Onegin.txt',\n",
    "    'Pushkin_A._Spisokshkolnoy._Polnoe_Sobranie_Stihotvoreniyi.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "\n",
    "for f in files:\n",
    "    text = text + '\\n' + open('txt/' + f, 'rt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Александр Александрович Блок\n",
      "\n",
      "Полное собрание стихотворений\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Стихотворения 1898 года\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "«Пусть светит месяц – ночь темна…»\n",
      "\n",
      "\n",
      "\t\tПусть светит месяц – ночь темна.\n",
      "\t\tПусть жизнь приносит людям счастье, —\n",
      "\t\tВ моей душе любви весна\n",
      "\t\tНе сменит бурного ненастья.\n",
      "\t\tНочь распростерлась надо мной\n",
      "\t\tИ отвечает мертвым взглядом\n",
      "\t\tНа тусклый взор души больной,\n",
      "\t\tОблитой острым, сладким ядом.\n",
      "\t\tИ тщетно, страсти затая,\n",
      "\t\tВ холодной мгле передрассветной\n",
      "\t\tСреди толпы блуждаю я\n",
      "\t\tС одной лишь думою завет\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nАлександр Александрович Блок\\n\\nПолное собрание стихотворений\\n\\n\\n\\n\\n\\nСтихотворения 1898 года\\n\\n\\n\\n\\n\\n«Пусть светит месяц – ночь темна…»\\n\\n\\n\\t\\tПусть светит месяц – ночь темна.\\n\\t\\tПусть жизнь приносит людям счастье, —\\n\\t\\tВ моей душе любви весна\\n\\t\\tНе сменит бурного ненастья.\\n\\t\\tНочь распростерлась надо мной\\n\\t\\tИ отвечает мертвым взглядом\\n\\t\\tНа тусклый взор души больной,\\n\\t\\tОблитой острым, сладким ядом.\\n\\t\\tИ тщетно, страсти затая,\\n\\t\\tВ холодной мгле передрассветной\\n\\t\\tСреди толпы блуждаю я\\n\\t\\tС одной лишь думою завет'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text[:500])\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_re = re.sub(r'\\xa0', ' ', text)\n",
    "text_re = re.sub(r'\\t', '', text_re)\n",
    "text_re = re.sub(r'[^а-яА-Я\\n ]+', '', text_re)\n",
    "text_re = re.sub(r'[ ]{2,}', ' ', text_re)\n",
    "text_re = re.sub(r' \\n', '\\n', text_re)\n",
    "text_re = re.sub(r'\\n ', '\\n', text_re)\n",
    "text_re = re.sub(r'[\\n]{3,}', '\\n\\n', text_re)\n",
    "text_re = re.sub(r'[\\n]{2,}[а-яА-Я ]+[\\n]{2,}', '\\n\\n', text_re)\n",
    "text_re = re.sub(r'[\\n]{2,}[а-яА-Я ]+[\\n]{2,}', '\\n\\n', text_re)\n",
    "text_re = re.sub(r'(^[\\n])[а-яА-Я ]+[\\n]{2,}', '', text_re)\n",
    "text = text_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пусть светит месяц ночь темна\n",
      "Пусть жизнь приносит людям счастье\n",
      "В моей душе любви весна\n",
      "Не сменит бурного ненастья\n",
      "Ночь распростерлась надо мной\n",
      "И отвечает мертвым взглядом\n",
      "На тусклый взор души больной\n",
      "Облитой острым сладким ядом\n",
      "И тщетно страсти затая\n",
      "В холодной мгле передрассветной\n",
      "Среди толпы блуждаю я\n",
      "С одной лишь думою заветной\n",
      "Пусть светит месяц ночь темна\n",
      "Пусть жизнь приносит людям счастье\n",
      "В моей душе любви весна\n",
      "Не сменит бурного ненастья\n",
      "\n",
      "Одной тебе тебе одной\n",
      "Любви и счастия царице\n",
      "Те\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "print(text[pos:pos+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "П\n",
      "у\n",
      "с\n",
      "т\n",
      "ь\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 50 #100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Пусть светит месяц ночь темна\\nПусть жизнь приносит '\n",
      "'людям счастье\\nВ моей душе любви весна\\nНе сменит бур'\n",
      "'ного ненастья\\nНочь распростерлась надо мной\\nИ отвеч'\n",
      "'ает мертвым взглядом\\nНа тусклый взор души больной\\nО'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(4):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Пусть светит месяц ночь темна\\nПусть жизнь приносит'\n",
      "Target data: 'усть светит месяц ночь темна\\nПусть жизнь приносит '\n",
      "Input data:  'людям счастье\\nВ моей душе любви весна\\nНе сменит бу'\n",
      "Target data: 'юдям счастье\\nВ моей душе любви весна\\nНе сменит бур'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(2):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 50), (128, 50)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 512\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "                                 \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                   \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50, 64) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 512)          32768     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1024)         4724736   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (128, None, 1024)         6297600   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (128, None, 1024)         6297600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 64)           65600     \n",
      "=================================================================\n",
      "Total params: 17,418,304\n",
      "Trainable params: 17,418,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ны\\nНа землю сумерки сходили\\nИ вечности вставали сн'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'фюпфЯвШИпнюЖЧЭчМпЯяАфжКкСЦаьЩюЩжАЭХБлФмСзьдЩХциВйэ'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rseme\\.conda\\envs\\tf2-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[[input_example_batch[0]]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 50, 64)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.157918\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    period=10,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "299/299 [==============================] - 30s 88ms/step - loss: 2.8211\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 27s 87ms/step - loss: 1.8721\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 1.7183\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 1.6479\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 27s 87ms/step - loss: 1.5970\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 26s 87ms/step - loss: 1.5502\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 26s 87ms/step - loss: 1.5082\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 26s 87ms/step - loss: 1.4690\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 26s 87ms/step - loss: 1.4269\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 27s 87ms/step - loss: 1.3867\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 27s 87ms/step - loss: 1.3479\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 27s 89ms/step - loss: 1.3102\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 26s 86ms/step - loss: 1.2733\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 27s 87ms/step - loss: 1.2405\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 27s 88ms/step - loss: 1.2076\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 27s 88ms/step - loss: 1.1803\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 27s 88ms/step - loss: 1.1527\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 27s 88ms/step - loss: 1.1279\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 27s 90ms/step - loss: 1.1060\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 27s 88ms/step - loss: 1.0854\n",
      "Wall time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_20'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 512)            32768     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           4724736   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 64)             65600     \n",
      "=================================================================\n",
      "Total params: 17,418,304\n",
      "Trainable params: 17,418,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature, num_generate):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    #num_generate = 500\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    #temperature = 0.1\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Златая цепь на дубе полной\n",
      "Благословлять ее не могла\n",
      "Довольно воспал в тревоге\n",
      "Непроницала\n",
      "\n",
      "И сердцем молод лавр и кипит\n",
      "От всех тревогой и прочь\n",
      "Что ж на сердце имена Свидетели мы теперь и молвил\n",
      "Посмотри желание мощи\n",
      "И не сойдет ли с досады гордый лорнет\n",
      "В степь и венца нет мочи\n",
      "И пленниц Петрова приятель\n",
      "Порох в три\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Златая цепь на дубе \", temperature=0.5, num_generate=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Геренерация на словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Пусть',\n",
       " ' ',\n",
       " 'светит',\n",
       " ' ',\n",
       " 'месяц',\n",
       " ' ',\n",
       " 'ночь',\n",
       " ' ',\n",
       " 'темна',\n",
       " '\\n',\n",
       " 'Пусть',\n",
       " ' ',\n",
       " 'жизнь',\n",
       " ' ',\n",
       " 'приносит',\n",
       " ' ',\n",
       " 'людям',\n",
       " ' ',\n",
       " 'счастье',\n",
       " '\\n']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tok = re.split(r'(\\s+)', text)\n",
    "text_tok[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67871 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text_tok))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пусть\n",
      " \n",
      "светит\n",
      " \n",
      "месяц\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 10\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Пусть светит месяц ночь темна\\nПусть'\n",
      "' жизнь приносит людям счастье\\nВ '\n",
      "'моей душе любви весна\\nНе сменит'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(3):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Пусть светит месяц ночь темна\\n'\n",
      "Target data: ' светит месяц ночь темна\\nПусть'\n",
      "Input data:  ' жизнь приносит людям счастье\\nВ'\n",
      "Target data: 'жизнь приносит людям счастье\\nВ '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 10), (128, 10)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 512\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 10, 67871) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (128, None, 512)          34749952  \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (128, None, 1024)         4724736   \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (128, None, 1024)         6297600   \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (128, None, 1024)         6297600   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (128, None, 67871)        69567775  \n",
      "=================================================================\n",
      "Total params: 121,637,663\n",
      "Trainable params: 121,637,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'проклятых числ\\n\\nМой монастырь где '\n",
      "\n",
      "Next Char Predictions: \n",
      " 'ПрободающихпутиноюсахаркаковаРешусьобведеныДнитатариномдобродетелипарнасский'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rseme\\.conda\\envs\\tf2-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[[input_example_batch[0]]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 10, 67871)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       11.125151\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    period=5,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "465/465 [==============================] - 128s 270ms/step - loss: 6.6247\n",
      "Epoch 2/5\n",
      "465/465 [==============================] - 125s 268ms/step - loss: 6.2726\n",
      "Epoch 3/5\n",
      "465/465 [==============================] - 126s 270ms/step - loss: 6.3533\n",
      "Epoch 4/5\n",
      "465/465 [==============================] - 128s 274ms/step - loss: 6.3658\n",
      "Epoch 5/5\n",
      "465/465 [==============================] - 127s 274ms/step - loss: 6.3688\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_5'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, None, 512)            34749952  \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (1, None, 1024)           4724736   \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, None, 67871)          69567775  \n",
      "=================================================================\n",
      "Total params: 121,637,663\n",
      "Trainable params: 121,637,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature, num_generate):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    #num_generate = 500\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    #temperature = 0.1\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (''.join(start_string) + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Златая цепь на дубе   \n",
      " \n",
      "   ночейКипя с\n",
      "  и \n",
      "искрой Конечно наНе ОндушоюпустынеИ\n",
      "  твоего    наша здесь переступившийчасовой низвергнутся ДухуяЧто\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=['Златая', ' ', 'цепь', ' ', 'на', ' ', 'дубе'], temperature=1, num_generate=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы:\n",
    "На словах обучить модель не получилось. Обучение застревает на loss ~ 6. Тогда как на буквах обучение проходит довольно гладко и loss ~ 1. Поэтому на буквах отработало лучше, но до идеала далековато."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Поэкспериментировать с переводом написать по вашим наблюдениям где он ошибается, попробовать изменить архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./content/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> несомненно , для каждого мужчины в этом мире где то есть подходящая женщина , которая может стать ему женой , обратное верно и для женщин . но если учесть , что у человека может быть максимум несколько сотен знакомых , из которых лишь дюжина , а то и меньше , тех , кого он знает близко , а из этой дюжины у него один или от силы два друга , то можно легко увидеть , что с уч том миллионов живущих на земле людей , ни один подходящий мужчина , возможно , ещ не встретил подходящую женщину . <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(ru[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 400000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000 320000 80000 80000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = \\\n",
    "    train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> я\n",
      "65 ----> уже\n",
      "6 ----> не\n",
      "762 ----> занята\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "25 ----> i'm\n",
      "39 ----> not\n",
      "226 ----> busy\n",
      "246 ----> anymore\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 512 #64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 512 #300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([512, 21]), TensorShape([512, 17]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (512, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        # self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 1024])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4998\n",
      "Epoch 1 Batch 100 Loss 1.9838\n",
      "Epoch 1 Batch 200 Loss 1.7446\n",
      "Epoch 1 Batch 300 Loss 1.6382\n",
      "Epoch 1 Batch 400 Loss 1.5669\n",
      "Epoch 1 Batch 500 Loss 1.5522\n",
      "Epoch 1 Batch 600 Loss 1.3938\n",
      "Epoch 1 Loss 1.7202\n",
      "Time taken for 1 epoch 200.65352416038513 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3411\n",
      "Epoch 2 Batch 100 Loss 1.1713\n",
      "Epoch 2 Batch 200 Loss 1.0809\n",
      "Epoch 2 Batch 300 Loss 0.9464\n",
      "Epoch 2 Batch 400 Loss 0.8598\n",
      "Epoch 2 Batch 500 Loss 0.7001\n",
      "Epoch 2 Batch 600 Loss 0.7043\n",
      "Epoch 2 Loss 0.9664\n",
      "Time taken for 1 epoch 186.5998249053955 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5863\n",
      "Epoch 3 Batch 100 Loss 0.5415\n",
      "Epoch 3 Batch 200 Loss 0.4690\n",
      "Epoch 3 Batch 300 Loss 0.5108\n",
      "Epoch 3 Batch 400 Loss 0.4495\n",
      "Epoch 3 Batch 500 Loss 0.4295\n",
      "Epoch 3 Batch 600 Loss 0.4037\n",
      "Epoch 3 Loss 0.4792\n",
      "Time taken for 1 epoch 188.19484949111938 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3163\n",
      "Epoch 4 Batch 100 Loss 0.2702\n",
      "Epoch 4 Batch 200 Loss 0.3003\n",
      "Epoch 4 Batch 300 Loss 0.3057\n",
      "Epoch 4 Batch 400 Loss 0.3011\n",
      "Epoch 4 Batch 500 Loss 0.2677\n",
      "Epoch 4 Batch 600 Loss 0.2722\n",
      "Epoch 4 Loss 0.2945\n",
      "Time taken for 1 epoch 186.43277287483215 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1867\n",
      "Epoch 5 Batch 100 Loss 0.1980\n",
      "Epoch 5 Batch 200 Loss 0.2166\n",
      "Epoch 5 Batch 300 Loss 0.2035\n",
      "Epoch 5 Batch 400 Loss 0.2023\n",
      "Epoch 5 Batch 500 Loss 0.1895\n",
      "Epoch 5 Batch 600 Loss 0.2027\n",
      "Epoch 5 Loss 0.2029\n",
      "Time taken for 1 epoch 186.92322063446045 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1486\n",
      "Epoch 6 Batch 100 Loss 0.1417\n",
      "Epoch 6 Batch 200 Loss 0.1547\n",
      "Epoch 6 Batch 300 Loss 0.1490\n",
      "Epoch 6 Batch 400 Loss 0.1573\n",
      "Epoch 6 Batch 500 Loss 0.1486\n",
      "Epoch 6 Batch 600 Loss 0.1422\n",
      "Epoch 6 Loss 0.1496\n",
      "Time taken for 1 epoch 185.82981300354004 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1012\n",
      "Epoch 7 Batch 100 Loss 0.1076\n",
      "Epoch 7 Batch 200 Loss 0.1099\n",
      "Epoch 7 Batch 300 Loss 0.1140\n",
      "Epoch 7 Batch 400 Loss 0.1195\n",
      "Epoch 7 Batch 500 Loss 0.1206\n",
      "Epoch 7 Batch 600 Loss 0.1324\n",
      "Epoch 7 Loss 0.1158\n",
      "Time taken for 1 epoch 185.36062598228455 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0838\n",
      "Epoch 8 Batch 100 Loss 0.0938\n",
      "Epoch 8 Batch 200 Loss 0.0849\n",
      "Epoch 8 Batch 300 Loss 0.0970\n",
      "Epoch 8 Batch 400 Loss 0.0907\n",
      "Epoch 8 Batch 500 Loss 0.1101\n",
      "Epoch 8 Batch 600 Loss 0.1002\n",
      "Epoch 8 Loss 0.0938\n",
      "Time taken for 1 epoch 187.69091176986694 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0773\n",
      "Epoch 9 Batch 100 Loss 0.0760\n",
      "Epoch 9 Batch 200 Loss 0.0734\n",
      "Epoch 9 Batch 300 Loss 0.0691\n",
      "Epoch 9 Batch 400 Loss 0.0814\n",
      "Epoch 9 Batch 500 Loss 0.0842\n",
      "Epoch 9 Batch 600 Loss 0.0977\n",
      "Epoch 9 Loss 0.0790\n",
      "Time taken for 1 epoch 185.71744346618652 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0532\n",
      "Epoch 10 Batch 100 Loss 0.0667\n",
      "Epoch 10 Batch 200 Loss 0.0668\n",
      "Epoch 10 Batch 300 Loss 0.0622\n",
      "Epoch 10 Batch 400 Loss 0.0730\n",
      "Epoch 10 Batch 500 Loss 0.0771\n",
      "Epoch 10 Batch 600 Loss 0.0770\n",
      "Epoch 10 Loss 0.0692\n",
      "Time taken for 1 epoch 185.22634077072144 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x263561ce788>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Перевод при изначальном запуске"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: <start> здесь хорошо . <end>\n",
    "#Predicted translation: it is good weather . <end> \n",
    "#Input: <start> я не смогу поехать . <end>\n",
    "#Predicted translation: i can't go . <end> \n",
    "#Input: <start> вы еще дома ? <end>\n",
    "#Predicted translation: are you still home ? <end> \n",
    "#Input: <start> вы все еще дома ? <end>\n",
    "#Predicted translation: are you still at home ? <end> \n",
    "#Input: <start> попробуй сделать это . <end>\n",
    "#Predicted translation: try to do it . <end> \n",
    "#Input: <start> я люблю , когда идет снег . <end>\n",
    "#Predicted translation: i like what i seem . <end> \n",
    "#Input: <start> я никогда такого не делаю . <end>\n",
    "#Predicted translation: i never do that . <end> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Перевод после улучшения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> здесь хорошо . <end>\n",
      "Predicted translation: this is good here . <end> \n",
      "Input: <start> я не смогу поехать . <end>\n",
      "Predicted translation: i can't go . <end> \n",
      "Input: <start> вы еще дома ? <end>\n",
      "Predicted translation: are you still home ? <end> \n",
      "Input: <start> вы все еще дома ? <end>\n",
      "Predicted translation: are you still at home ? <end> \n",
      "Input: <start> попробуй сделать это . <end>\n",
      "Predicted translation: try to do it . <end> \n",
      "Input: <start> я люблю , когда идет снег . <end>\n",
      "Predicted translation: i like it when it snows . <end> \n",
      "Input: <start> я никогда такого не делаю . <end>\n",
      "Predicted translation: i never do that . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Здесь хорошо.')\n",
    "translate('Я не смогу поехать.')\n",
    "translate(u'Вы еще дома?')\n",
    "translate(u'Вы все еще дома?')\n",
    "translate(u'Попробуй сделать это.')\n",
    "translate(u'Я люблю, когда идет снег.')\n",
    "translate(u'Я никогда такого не делаю.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы:\n",
    "Увеличил размер обучаемого датасета и размер эмбединга. В результате явно неправильные переводы улучшились.<br>\n",
    "Например:<br>\n",
    "для \"здесь хорошо\" было \"it is good weather\", а стало \"this is good here\",<br>\n",
    "для \"я люблю , когда идет снег\" было \"i like what i seem\", а стало \"i like it when it snows\".<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
